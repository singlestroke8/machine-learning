## 3 自動EDA

### 本プロセスの目的:

未知のデータセットに対して手作業で一つ一つの変数を可視化する「車輪の再発明」を避け、ydata-profiling を用いて基礎集計・分布確認・相関分析を自動化する。これにより、エンジニアの貴重な時間を「データからビジネス上の仮説（なぜ解約されるのか？）を立てる」という本質的な作業に集中させる。

### ポイント:

* データ品質の把握: 欠損値の割合や、カーディナリティ（値の種類）が極端に高い・低い変数を瞬時に特定できる。
* ターゲット変数（Churn）との関係の俯瞰: どの特徴量が解約（Churn）と強い相関を持っているかの初期仮説を得られる。
* 再現性の担保: スクリプト化することで、データが更新された際もコマンド一つでレポートを再生成できる運用を想定している。

## 4 個別EDA

### 本プロセスの目的:

自動生成ツール（ydata-profiling）で得たデータの全体像や初期仮説をもとに、ビジネス課題（今回の場合は「解約の防止」）に直結する重要な特徴量に絞って深掘りし、ステークホルダー（非エンジニアの意思決定者）に説明するための視覚的な証拠（グラフ画像）を作成する。

### ポイント:

* 仮説の検証と可視化: 「契約期間が短いほど解約されやすいのではないか？」といった仮説を、具体的なデータ（分布や割合）を用いてグラフ化し、客観的な事実として裏付ける。
* ビジネスインサイトの抽出: 単なるデータの羅列ではなく、グラフから「初期のオンボーディング（定着支援）が最重要課題である」「月額契約から年間契約への誘導施策が有効である」といった、具体的なビジネスアクションに繋がる示唆を得る。
* レポーティングの自動化・再現性: Jupyter Notebook上に描画して終わるのではなく、.pyスクリプトとして実装し画像をファイル保存することで、データが更新された際もコマンド一つで報告用資料（.png）を再生成できる保守性の高い運用を実現する。

### 5 特徴量生成

### 本プロセスの目的:

既存の生データ（Raw Data）の項目をそのまま機械学習モデルに投入するのではなく、EDAで得られたビジネス上の仮説に基づき、モデルが「解約の予兆」を学習しやすくなるような新しい変数を意図的に作り出す（特徴量エンジニアリング）。生成したデータは、元データとは分離して中間データ（Interim Data）として保存する。

### ポイント:

* ドメイン知識と仮説の適用: 「セキュリティやバックアップなどの付加サービスを多く契約している顧客は、他社への乗り換え（解約）コストが高くなり解約しづらいのではないか？」といった仮説を立て、それらを合計した新しい変数（例：Num_Additional_Services）を作成する。

* 実務特有の「隠れたノイズ」の処理: 一見数値に見える列（TotalCharges：総支払額）に、新規顧客特有の「空白スペース」が混入して文字列型（Object）になっているという実務あるあるの罠を検知し、適切に数値型（Float）に変換・補完する処理を組み込む。

* 生データの不変性（Immutability）の担保: 処理後のデータを data/raw/ に上書きするのではなく、data/interim/ （中間ディレクトリ）に別ファイルとして出力することで、データパイプラインの安全性と再現性を確保する。

### 6　前処理

### 本プロセスの目的:

中間データ（人間が理解しやすい状態のデータ）を、機械学習アルゴリズムが計算可能な「数値の配列」に変換する。この際、単にデータを変換するだけでなく、将来のWeb API化（推論フェーズ）を見据え、**「全く同じ変換処理を、未知の新規データに対しても適用できるモジュール（部品）」**として設計・保存する。

### ポイント:

* データリーク（Data Leakage）の防止

スケーリング（標準化）やエンコーディングを行う前に、必ずデータを学習用（Train）とテスト用（Test）に分割する。全データでスケーリングをしてしまうと、未来のデータ（テストデータ）の情報が学習時に漏れ出てしまい、実務において致命的な精度の過大評価を引き起こすため、この順序を守ることがプロの証明になる。

* ColumnTransformerによる処理の統合

数値データには StandardScaler（標準化）、カテゴリデータには OneHotEncoder（ダミー変数化）と、列の型によって異なる処理を scikit-learn の機能を使って美しく一つのパイプラインにまとめる。

* 前処理器（Preprocessor）の保存

学習データに適合（Fit）させた変換ルール（平均値や標準偏差、カテゴリの分類基準など）を .pkl ファイルとして保存する。これにより、Week 4のFastAPIで新しいユーザーデータが送られてきた際、このファイルを読み込むだけで瞬時に同じ前処理を適用できる。

### 7 パイプライン

### 本プロセスの目的:

構築した「前処理（スケーリングやエンコーディングのルール）」と、これから構築する「機械学習アルゴリズム（推論）」を、Scikit-learnの Pipeline クラスを用いて**「1つの直列な処理フロー（筒）」**として結合する。これにより、生のデータを入力するだけで「前処理」→「予測」までが自動で完結する堅牢なシステムを作り上げる。

### ポイント:

### 8 ベースライン

### 本プロセスの目的:

複雑で高度なアルゴリズム（LightGBMやニューラルネットワークなど）を導入する前に、まずは数学的にシンプルで解釈性の高いモデル（ロジスティック回帰など）を用いて**「最低限越えるべき予測精度の基準（ベースライン）」**を確立する。同時に、前処理から評価までの一連のパイプラインがバグなく疎通しているかを検証する。

### ポイント:

* 「ベンチマーク」の策定と課題の明確化:
いきなり複雑なモデルを使うと、その精度が「データが良いから」なのか「モデルが優れているから」なのか判断できません。ベースラインを敷くことで、「解釈性の高いシンプルなモデルでもAUC 0.84は出るが、解約者の見落とし（Recall 55.8%）に課題が残る」といった**「次に解くべきビジネス上のボトルネック」**を論理的に浮き彫りにすることができます。

* ビジネスサイドへの「解釈性（Explainability）」の担保:
ロジスティック回帰や決定木のようなシンプルなアルゴリズムは、「どの特徴量が、予測にプラス／マイナスに働いたか（偏回帰係数）」を人間が完全に理解し、説明することができます。「ブラックボックスな高精度AI」よりも、「多少精度が落ちても理由が説明できるモデル」の方が実務（特に金融や医療、経営層への報告）では好まれるケースが多く、その選択肢を提示できる能力の証明になります。

* オーバーエンジニアリングの回避:
もしシンプルなモデルでビジネスの要求水準を満たせるのであれば、計算コストや運用保守のコストが高い複雑なアルゴリズムを使う必要はありません。この「費用対効果を見極める姿勢」は、単なるAI開発者ではなく、ビジネスに貢献する機械学習エンジニアとして高く評価されます。