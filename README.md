## 3 自動EDA

### 本プロセスの目的:

未知のデータセットに対して手作業で一つ一つの変数を可視化する「車輪の再発明」を避け、ydata-profiling を用いて基礎集計・分布確認・相関分析を自動化する。これにより、エンジニアの貴重な時間を「データからビジネス上の仮説（なぜ解約されるのか？）を立てる」という本質的な作業に集中させる。

### ポイント:

* データ品質の把握: 欠損値の割合や、カーディナリティ（値の種類）が極端に高い・低い変数を瞬時に特定できる。
* ターゲット変数（Churn）との関係の俯瞰: どの特徴量が解約（Churn）と強い相関を持っているかの初期仮説を得られる。
* 再現性の担保: スクリプト化することで、データが更新された際もコマンド一つでレポートを再生成できる運用を想定している。

## 4 個別EDA

### 本プロセスの目的:

自動生成ツール（ydata-profiling）で得たデータの全体像や初期仮説をもとに、ビジネス課題（今回の場合は「解約の防止」）に直結する重要な特徴量に絞って深掘りし、ステークホルダー（非エンジニアの意思決定者）に説明するための視覚的な証拠（グラフ画像）を作成する。

### ポイント:

* 仮説の検証と可視化: 「契約期間が短いほど解約されやすいのではないか？」といった仮説を、具体的なデータ（分布や割合）を用いてグラフ化し、客観的な事実として裏付ける。
* ビジネスインサイトの抽出: 単なるデータの羅列ではなく、グラフから「初期のオンボーディング（定着支援）が最重要課題である」「月額契約から年間契約への誘導施策が有効である」といった、具体的なビジネスアクションに繋がる示唆を得る。
* レポーティングの自動化・再現性: Jupyter Notebook上に描画して終わるのではなく、.pyスクリプトとして実装し画像をファイル保存することで、データが更新された際もコマンド一つで報告用資料（.png）を再生成できる保守性の高い運用を実現する。

### 5 特徴量生成

### 本プロセスの目的:

既存の生データ（Raw Data）の項目をそのまま機械学習モデルに投入するのではなく、EDAで得られたビジネス上の仮説に基づき、モデルが「解約の予兆」を学習しやすくなるような新しい変数を意図的に作り出す（特徴量エンジニアリング）。生成したデータは、元データとは分離して中間データ（Interim Data）として保存する。

### ポイント:

* ドメイン知識と仮説の適用: 「セキュリティやバックアップなどの付加サービスを多く契約している顧客は、他社への乗り換え（解約）コストが高くなり解約しづらいのではないか？」といった仮説を立て、それらを合計した新しい変数（例：Num_Additional_Services）を作成する。

* 実務特有の「隠れたノイズ」の処理: 一見数値に見える列（TotalCharges：総支払額）に、新規顧客特有の「空白スペース」が混入して文字列型（Object）になっているという実務あるあるの罠を検知し、適切に数値型（Float）に変換・補完する処理を組み込む。

* 生データの不変性（Immutability）の担保: 処理後のデータを data/raw/ に上書きするのではなく、data/interim/ （中間ディレクトリ）に別ファイルとして出力することで、データパイプラインの安全性と再現性を確保する。

### 6　前処理

### 本プロセスの目的:

中間データ（人間が理解しやすい状態のデータ）を、機械学習アルゴリズムが計算可能な「数値の配列」に変換する。この際、単にデータを変換するだけでなく、将来のWeb API化（推論フェーズ）を見据え、**「全く同じ変換処理を、未知の新規データに対しても適用できるモジュール（部品）」**として設計・保存する。

### ポイント:

* データリーク（Data Leakage）の防止

スケーリング（標準化）やエンコーディングを行う前に、必ずデータを学習用（Train）とテスト用（Test）に分割する。全データでスケーリングをしてしまうと、未来のデータ（テストデータ）の情報が学習時に漏れ出てしまい、実務において致命的な精度の過大評価を引き起こすため、この順序を守ることがプロの証明になる。

* ColumnTransformerによる処理の統合

数値データには StandardScaler（標準化）、カテゴリデータには OneHotEncoder（ダミー変数化）と、列の型によって異なる処理を scikit-learn の機能を使って美しく一つのパイプラインにまとめる。

* 前処理器（Preprocessor）の保存

学習データに適合（Fit）させた変換ルール（平均値や標準偏差、カテゴリの分類基準など）を .pkl ファイルとして保存する。これにより、Week 4のFastAPIで新しいユーザーデータが送られてきた際、このファイルを読み込むだけで瞬時に同じ前処理を適用できる。

### 7 パイプライン

### 本プロセスの目的:

構築した「前処理（スケーリングやエンコーディングのルール）」と、これから構築する「機械学習アルゴリズム（推論）」を、Scikit-learnの Pipeline クラスを用いて**「1つの直列な処理フロー（筒）」**として結合する。これにより、生のデータを入力するだけで「前処理」→「予測」までが自動で完結する堅牢なシステムを作り上げる。

### ポイント:

### 8 ベースライン

### 本プロセスの目的:

複雑で高度なアルゴリズム（LightGBMやニューラルネットワークなど）を導入する前に、まずは数学的にシンプルで解釈性の高いモデル（ロジスティック回帰など）を用いて**「最低限越えるべき予測精度の基準（ベースライン）」**を確立する。同時に、前処理から評価までの一連のパイプラインがバグなく疎通しているかを検証する。

### ポイント:

* 「ベンチマーク」の策定と課題の明確化:
いきなり複雑なモデルを使うと、その精度が「データが良いから」なのか「モデルが優れているから」なのか判断できません。ベースラインを敷くことで、「解釈性の高いシンプルなモデルでもAUC 0.84は出るが、解約者の見落とし（Recall 55.8%）に課題が残る」といった**「次に解くべきビジネス上のボトルネック」**を論理的に浮き彫りにすることができます。

* ビジネスサイドへの「解釈性（Explainability）」の担保:
ロジスティック回帰や決定木のようなシンプルなアルゴリズムは、「どの特徴量が、予測にプラス／マイナスに働いたか（偏回帰係数）」を人間が完全に理解し、説明することができます。「ブラックボックスな高精度AI」よりも、「多少精度が落ちても理由が説明できるモデル」の方が実務（特に金融や医療、経営層への報告）では好まれるケースが多く、その選択肢を提示できる能力の証明になります。

* オーバーエンジニアリングの回避:
もしシンプルなモデルでビジネスの要求水準を満たせるのであれば、計算コストや運用保守のコストが高い複雑なアルゴリズムを使う必要はありません。この「費用対効果を見極める姿勢」は、単なるAI開発者ではなく、ビジネスに貢献する機械学習エンジニアとして高く評価されます。

### 8 モデル選定

### 本プロセスの目的:

ベースラインモデル（ロジスティック回帰など）で浮き彫りになった「解約者の見落とし（低いRecall）」というビジネス上の課題を克服するため、実務やKaggleで最強クラスの予測精度を誇るLightGBM（勾配ブースティング木）を導入する。
同時に、モデルの精度が「たまたまテストデータの分割運が良かっただけ」ではないことを客観的に証明するため、クロスバリデーション（交差検証）を実装し、未知のデータに対する真の汎化性能（過学習していないこと）を厳密に評価する。

### ポイント:

* LightGBM（勾配ブースティング木）の威力:
ランダムフォレストと同じ決定木の仲間ですが、「前の木が間違えた（予測を外した）データに対して、次の木が重点的に修正をかける」というバケツリレーのような直列の学習（ブースティング）を行うため、圧倒的な精度と計算スピードを叩き出します。現在のテーブルデータ分析におけるデファクトスタンダード（業界標準）です。

* 不均衡データの克服（ビジネス指標の改善）:
今回のビジネス課題である「解約者（少数派）の見落とし」を防ぐため、アルゴリズムに「解約者の予測を外したときのペナルティを重くする（class_weight='balanced'）」というパラメータを設定します。これにより、ただ正解率（Accuracy）を追うのではなく、ビジネス価値に直結するRecall（再現率）を意図的に引き上げるという「目的逆算型のモデリング」を実現します。

* クロスバリデーション（CV）による信頼性の担保:
データを例えば5分割（5-Fold）し、「4つで学習し、1つでテストする」というサイクルを5回繰り返し、その平均スコアを算出します。実務では、1回限りのテストスコアではなく、この「CVスコア」こそが本番環境での真の実力としてステークホルダーに報告されます。

### 8 チューニング

### 本プロセスの目的:

選定したチャンピオンモデル（LightGBM）の真のポテンシャルを引き出すため、先進的な最適化ライブラリである Optuna を導入し、ハイパーパラメータ（学習率や木の深さ、葉の数など）の最適な組み合わせを自動探索する。モデルを「汎用的なデフォルト設定」から「今回のデータセット（解約予測）に特化した専用設定」へと昇華させ、ビジネスKPIの最大化を図る。

### ポイント:

* 最先端の最適化アルゴリズム（ベイズ最適化）の活用:
従来の非効率な総当たり戦（Grid Search）やランダム探索ではなく、Optunaのベイズ最適化（TPEアルゴリズム）を採用。これは「過去の探索結果から学び、次に試すべき有望なパラメータを推測する」手法であり、計算コストを抑えながら高速に高精度なモデルを構築できるモダンな設計力のアピールとなる。

* ビジネス指標（F1-score / Recall）に直結した目的関数の設計:
Optunaが最大化を目指す「目的関数（Objective Function）」を、単なるAccuracy（正解率）ではなく、先ほどの検証で重要性が浮き彫りになった 「不均衡データにおける総合力（F1-score）」 または 「解約者の発見率（Recall）」 に設定。アルゴリズムに「ビジネス上の目的」を明確に指示するチューニングを実現した。

* 交差検証（CV）ループとの統合による「チューニングの過学習」防止:
パラメータをいじりすぎると、手元の学習データにだけ過剰に適合してしまうリスクがある。これを防ぐため、Optunaの探索ループの中にも「5-Fold CV（交差検証）」を組み込み、「未知のデータに対しても汎用的に通用するパラメータ」のみを厳密に選び抜く、実務水準の堅牢なパイプラインを構築した。

### 特徴量重要度（Feature Importance）を算出し、どの項目がマッチングに効いているかを画像として出力・確認する。

### 本プロセスの目的:

構築したチャンピオンモデル（LightGBM）の内部構造を解析し、**「どの特徴量（顧客属性やサービス利用状況）が解約に最も強い影響を与えているか」を可視化（Feature Importanceの算出）**する。これにより、AIを単なる「ブラックボックスな予測マシーン」で終わらせず、経営陣やマーケティング部門が「解約防止のための具体的なアクション」を起こすためのビジネスインサイト（洞察）を提供する。

### ポイント:

* モデルの解釈性（Explainability / XAI）の担保:
決定木ベースのアンサンブル学習（LightGBMなど）は高精度な反面、推論のプロセスが人間には理解しづらい「ブラックボックス」になりがちである。特徴量重要度を棒グラフなどで視覚化することで、ステークホルダーに対するAIの透明性と納得感を高め、実業務への導入ハードルを下げる工夫を行った。

* データ主導の意思決定（Data-Driven Action）への接続:
例えば「月額契約（Month-to-month）」や「光ファイバー回線（Fiber optic）」の重要度が高いことが判明した場合、「長期契約への移行キャンペーンを打つ」「該当回線の通信品質やサポート体制を見直す」といった具体的なビジネス施策に直結させることができる。「AIで予測して終わり」ではなく、「AIでビジネス課題を解決する」という視点を持っていることの証明となる。

* 前処理パイプラインからの「特徴量名」の完全な復元（エンジニアリング力）:
Scikit-learnの ColumnTransformer（ダミー変数化やスケーリング）を通したデータは、システム内部で列名を持たない純粋な行列（NumPy配列）に変換されてしまうため、そのままでは「どの列が重要だったのか」が人間に判別できない。この技術的課題に対し、エンコーダーから変換後のカラム名を動的に抽出し、重要度スコアと正確に紐づけてグラフ化する堅牢なコードを実装した。

### 10 リファクタ

### 本プロセスの目的:

これまで各スクリプト（train.pyやtune.pyなど）に分散・重複して記述されていた処理（データの読み込み、前処理の定義、モデルの保存など）を、「単一責任の原則（SRP: Single Responsibility Principle）」に基づいて共通のモジュール（部品）としてクラス化・関数化する。これにより、コードの重複を排除し（DRY原則）、今後のAPI化や保守・拡張が容易な堅牢なシステム設計を構築する。

### ポイント:

* 単一責任の原則（SRP）の適用

* DRY（Don't Repeat Yourself）原則

* 「API化（FastAPI）」に向けた必須の準備:
推論用のWeb API（FastAPI）構築の際、ベタ書きのスクリプトのままだと「APIから前処理器だけを綺麗に呼び出す」ことができません。機能を部品化（モジュール化）しておくことで、API側から from src.features import build_preprocessor のように必要な部品だけをブロックのように安全にインポートできるようになる。

### 11 型ヒント

### 本プロセスの目的:

開発したすべての関数やメソッドに対し、引数（入力）と戻り値（出力）の「型（Type）」を明記する型ヒント（Type Hinting）を徹底する。動的型付け言語であるPythonの弱点（実行するまで変数の中にどんなデータが入っているか分からない）を克服し、チーム開発や本番運用（API化）に耐えうる「安全で、可読性が高く、バグの起きにくい堅牢なコードベース」を構築する。

### ポイント:

* チーム開発における「自己文書化（Self-documenting）」:
例えば def predict(data): という関数名では、data にリストを渡すべきかPandasのDataFrameを渡すべきか、第三者には分かりません。これを def predict(data: pd.DataFrame) -> np.ndarray: と型ヒントをつけるだけで、コード自体がドキュメントの役割を果たし、他のエンジニアが迷わず使えるようになります。

* 致命的なバグの未然防止（静的エラーチェック）:
型を明記しておくことで、本来文字列（str）を渡すべき場所に数値（int）を渡してしまうようなミスを、プログラムを「実行する前」にVS Codeなどのエディタが波線で警告してくれます。これにより、本番環境での予期せぬクラッシュ（ランタイムエラー）を大幅に減らすことができます。

* 開発体験（DX）と生産性の劇的な向上:
型が明示されていると、エディタ（VS Code）が「あ、この変数はDataFrameだな」と認識し、.drop() や .groupby() といったメソッドを正確にサジェスト（自動補完）してくれるようになります。これにより、タイポ（打ち間違い）が減り、コーディングのスピードが圧倒的に速くなります。

