import os
import logging
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
# éç·šå½¢ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ
from sklearn.ensemble import RandomForestClassifier

# ãƒ­ã‚®ãƒ³ã‚°ã®è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

def build_pipeline(numeric_features: list, categorical_features: list) -> Pipeline:
    """å‰å‡¦ç†ã¨ãƒ¢ãƒ‡ãƒ«ã‚’çµåˆã—ãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹"""
    
    # 1. å‰å‡¦ç†å™¨ã®å®šç¾© (Day 8ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯)
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), numeric_features),
            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
        ],
        remainder='passthrough'
    )

    # 2. ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®šç¾© (å‰å‡¦ç† -> ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°)
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', LogisticRegression(random_state=42, max_iter=1000))
    ])
    
    return pipeline

# å‰å‡¦ç†å™¨ã®å®šç¾©ã‚’é–¢æ•°åŒ–ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ«ãƒ¼ãƒ—å†…ã§å†åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
def build_preprocessor(numeric_features: list, categorical_features: list) -> ColumnTransformer:
    return ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), numeric_features),
            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
        ],
        remainder='passthrough'
    )

# è©•ä¾¡ã¨ãƒ­ã‚°å‡ºã—ã‚’è¡Œã†å°‚ç”¨ã®é–¢æ•°
def evaluate_and_log(y_true, y_pred, y_proba, model_name: str) -> dict:
    """è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—ã—ã€ãƒ­ã‚°ã«å‡ºåŠ›ã™ã‚‹"""
    acc = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred)
    rec = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    auc = roc_auc_score(y_true, y_proba)
    
    logging.info(f"--- {model_name} è©•ä¾¡çµæœ ---")
    logging.info(f"Accuracy (æ­£è§£ç‡): {acc:.4f}")
    logging.info(f"Precision(é©åˆç‡): {prec:.4f}")
    logging.info(f"Recall   (å†ç¾ç‡): {rec:.4f}")
    logging.info(f"F1-score (F1å€¤) : {f1:.4f}")
    logging.info(f"ROC-AUC  (AUCå€¤) : {auc:.4f}\n")
    
    return {"acc": acc, "prec": prec, "rec": rec, "f1": f1, "auc": auc}

def main() -> None:
    input_file = "data/interim/features.csv"
    output_model_path = "models/baseline_pipeline.pkl"
    
    os.makedirs("models", exist_ok=True)

    try:
        logging.info(f"ä¸­é–“ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿: {input_file}")
        df = pd.read_csv(input_file)

        # ç‰¹å¾´é‡(X)ã¨ç›®çš„å¤‰æ•°(y)ã«åˆ†å‰²
        target_col = 'Churn'
        X = df.drop(columns=[target_col, 'customerID'])
        y = df[target_col].map({'Yes': 1, 'No': 0})

        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        logging.info(f"ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Œäº†: Train({X_train.shape[0]}è¡Œ), Test({X_test.shape[0]}è¡Œ)")

        # ã‚«ãƒ©ãƒ ã®å‹ã‚’è‡ªå‹•åˆ¤åˆ¥
        numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()
        categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()

        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰
        # pipeline = build_pipeline(numeric_features, categorical_features)

        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã‚’å­¦ç¿’ï¼ˆå†…éƒ¨ã§å‰å‡¦ç†ã®Fitã¨ãƒ¢ãƒ‡ãƒ«ã®FitãŒé †ç•ªã«å®Ÿè¡Œã•ã‚Œã‚‹ï¼‰
        logging.info("ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆå‰å‡¦ç†ï¼‹ãƒ¢ãƒ‡ãƒ«ï¼‰ã®å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
        # pipeline.fit(X_train, y_train)

        # 
        preprocessor = build_preprocessor(numeric_features, categorical_features)

        # è¾æ›¸ã«è©°ã‚ã¦forãƒ«ãƒ¼ãƒ—ã§å›ã™
        models = {
            "Logistic Regression": LogisticRegression(random_state=42, max_iter=1000),
            "Random Forest": RandomForestClassifier(random_state=42, n_estimators=100)
        }

        # ãƒ«ãƒ¼ãƒ—ã®å¤–ã§ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã‚’ä¿æŒã™ã‚‹å¤‰æ•°ã‚’æº–å‚™
        best_auc = 0
        best_model_name = ""
        best_pipeline = None

        logging.info("å„ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨è©•ä¾¡ã‚’é–‹å§‹ã—ã¾ã™ã€‚\n" + "="*40)

        for name, model in models.items():
            pipeline = Pipeline(steps=[
                ('preprocessor', preprocessor),
                ('classifier', model)
            ])
            
            # å­¦ç¿’ã¨æ¨è«–
            pipeline.fit(X_train, y_train)
            y_pred = pipeline.predict(X_test)
            y_proba = pipeline.predict_proba(X_test)[:, 1]
            
            # è©•ä¾¡
            metrics = evaluate_and_log(y_test, y_pred, y_proba, name)
            
            # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®åˆ¤å®šï¼ˆä»Šå›ã¯AUCã‚’åŸºæº–ã¨ã™ã‚‹ï¼‰
            if metrics["auc"] > best_auc:
                best_auc = metrics["auc"]
                best_model_name = name
                best_pipeline = pipeline

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬ï¼ˆå†…éƒ¨ã§å‰å‡¦ç†ã®Transformã¨ãƒ¢ãƒ‡ãƒ«ã®PredictãŒé †ç•ªã«å®Ÿè¡Œã•ã‚Œã‚‹ï¼‰
        # y_pred = pipeline.predict(X_test)
        # y_proba = pipeline.predict_proba(X_test)[:, 1] # AUCç®—å‡ºç”¨ï¼ˆã‚¯ãƒ©ã‚¹1ã®ç¢ºç‡ï¼‰

        # # è©•ä¾¡æŒ‡æ¨™ã®ç®—å‡º
        # acc = accuracy_score(y_test, y_pred)
        # prec = precision_score(y_test, y_pred)
        # rec = recall_score(y_test, y_pred)
        # f1 = f1_score(y_test, y_pred)
        # auc = roc_auc_score(y_test, y_proba)

        # logging.info("=== ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ï¼ˆLogistic Regressionï¼‰è©•ä¾¡çµæœ ===")
        # logging.info(f"Accuracy (æ­£è§£ç‡): {acc:.4f}")
        # logging.info(f"Precision(é©åˆç‡): {prec:.4f}")
        # logging.info(f"Recall   (å†ç¾ç‡): {rec:.4f}")
        # logging.info(f"F1-score (F1å€¤) : {f1:.4f}")
        # logging.info(f"ROC-AUC  (AUCå€¤) : {auc:.4f}")
        # logging.info("==================================================")

        logging.info("="*40)
        logging.info(f"ğŸ† æœ€ã‚‚AUCãŒé«˜ã‹ã£ãŸãƒ¢ãƒ‡ãƒ«: {best_model_name} (AUC: {best_auc:.4f})")

        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã”ã¨ä¿å­˜ï¼ˆæœ¬ç•ªç’°å¢ƒã®APIã§ã¯ã“ã‚Œã‚’èª­ã¿è¾¼ã‚€ã ã‘ã§æ¸ˆã‚€ï¼‰
        model_path = "models/baseline_pipeline.pkl"
        joblib.dump(pipeline, output_model_path)
        # logging.info(f"å­¦ç¿’æ¸ˆã¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_model_path}")
        logging.info(f"æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {model_path}")

    except Exception as e:
        logging.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

if __name__ == "__main__":
    main()